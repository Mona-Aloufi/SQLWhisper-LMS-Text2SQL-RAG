# requirements.txt
datasets
accelerate             # Essential for fast/efficient finetuning (e.g., DeepSpeed/QLoRA)
peft                   # Parameter-Efficient Finetuning (QLoRA/LoRA)
trl                    # Transformer Reinforcement Learning (for SFTTrainer)
bitsandbytes           # For 4-bit quantization (running large models on limited VRAM)

sqlalchemy             # Python SQL toolkit/ORM (Used for connecting to and querying the LMS DB)
psycopg2-binary        # PostgreSQL adapter (Use this if your LMS is on Postgres)

huggingface_hub        # For deploying the finetuned model to the Hugging Face Hub
pandas
numpy
tqdm
transformers>=4.35.0
torch>=2.0.0
python-dotenv>=1.0.0
accelerate>=0.24.0
sentencepiece>=0.1.99
protobuf>=3.20.0

